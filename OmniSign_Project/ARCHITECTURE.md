# OmniSign: Multilingual Bidirectional Communication Framework

## ğŸ¯ Project Vision
A Multilingual, Bidirectional Communication Platform that enables real-time, two-way conversation between sign language users and speakers of other languages (e.g., Malayalam â†” ASL). Unlike existing systems that only recognize isolated signs, OmniSign handles continuous sign sequences with contextual understanding.

---

## ğŸ—ï¸ System Architecture

### Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            BIDIRECTIONAL COMMUNICATION FLOW                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SIGNER INPUT â†’ MediaPipe Extraction â†’ Dual-Stream Processing â†’ Translation â†’ Text/Audio Output
   â†‘                                                                              â†“
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Signer-Adaptive Fine-Tuning â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SPEAKER INPUT â†’ Speech/Text â†’ Multilingual Translation (Pivot: English) â†’ Sign Video Generation
```

### Core Components

#### 1. **Dual-Stream Architecture**

**Manual Stream (LSTM-based):**
- Processes hand landmarks (21 keypoints per hand Ã— 2 hands = 42 inputs)
- Temporal modeling using Bidirectional LSTM
- Captures hand shape, position, and movement patterns
- Output: 256 hidden units

**Non-Manual Stream (CNN-based):**
- Facial expressions: 468 facial landmarks
- Body posture: 33 body keypoints
- Extracts spatial relationships and intensity
- Output: 128 features

**Fusion Layer:**
- Concatenates both streams (256 + 128 = 384 features)
- Dense layer with dropout for regularization
- Output: 256 features

**CTC Layer:**
- Handles continuous sequences without explicit frame-level alignment
- Allows variable-length input sequences
- Output: Softmax probabilities over action vocabulary

---

#### 2. **Pivot-Language Architecture**

```
Malayalam â†’ English (Google Translate) â†’ Sign Language (MediaPipe Generation)
   â†‘                                              â†“
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Signer Output (English) â—„â”€â”€â”€â”€â”€â”€â”˜
```

- **Central Hub:** English as the pivot language
- **Google Cloud APIs:** Used for real-time translation
- **Efficiency:** Reduces the need for direct language-to-language models

---

#### 3. **Signer-Adaptive Personalized Learning**

**Problem:** Every signer has unique hand shapes, speeds, and styles. Standard models fail to generalize.

**Solution:** Incremental fine-tuning
- Collect 5-10 personalized samples from each new user
- Fine-tune the last 2 Dense layers using low learning rate (1e-4)
- Update stored user profile with new patterns
- Confidence threshold: If model confidence < 0.85, ask for repeat + fine-tune

**User Profile Storage:**
```json
{
  "user_id": "user_001",
  "language": "Malayalam",
  "adapted_weights": {...},
  "calibration_samples": 15,
  "confidence_threshold": 0.85,
  "last_updated": "2025-12-19"
}
```

---

## ğŸ“Š Data Pipeline

### Data Collection
- **Framework:** MediaPipe Holistic
- **Per Frame:** 
  - Hand landmarks: 21 Ã— 2 hands = 42 keypoints (x, y, z, confidence)
  - Facial landmarks: 468 keypoints (subset used: ~100)
  - Body landmarks: 33 keypoints
  - **Total:** ~500 features per frame

- **Sequence:** 30 frames per gesture (adjustable)
- **Vocabulary:** 5 actions initially (Hello, Goodbye, Thank you, How are you, I need help)

### Data Format
```
MP_Data/
â”œâ”€â”€ Hello/
â”‚   â”œâ”€â”€ 0/
â”‚   â”‚   â”œâ”€â”€ 0.npy (shape: (30, 500))  # Sequence of 30 frames, 500 features each
â”‚   â”‚   â”œâ”€â”€ 1.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ 1/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Goodbye/
â””â”€â”€ ...
```

### Preprocessing
1. **Normalization:** Normalize landmarks to range [0, 1]
2. **Interpolation:** Handle missing frames using spline interpolation
3. **Augmentation:** 
   - Random flipping (left-right)
   - Random temporal shifts
   - Random scaling (Â±5%)
   - Gaussian noise (Ïƒ=0.01)

---

## ğŸ§  Model Architecture

### Layer-by-Layer Specification

```
Input Shape: (30, 500)  # 30 frames, 500 features per frame

MANUAL STREAM:
  â”œâ”€ Hand Features: (30, 168)  # 21 pts Ã— 2 hands Ã— 4 (x,y,z,conf)
  â”œâ”€ Bidirectional LSTM (256 units)
  â”‚  â”œâ”€ Forward: 256 units
  â”‚  â””â”€ Backward: 256 units
  â”‚  Output: (30, 512)
  â””â”€ GlobalAveragePooling1D â†’ (512,)

NON-MANUAL STREAM:
  â”œâ”€ Facial + Body Features: (30, 332)  # 100 facial + 33 body Ã— 4
  â”œâ”€ Conv1D (filters=64, kernel=3) â†’ ReLU
  â”œâ”€ MaxPooling1D(2)
  â”œâ”€ Conv1D (filters=128, kernel=3) â†’ ReLU
  â”œâ”€ GlobalAveragePooling1D â†’ (128,)
  â””â”€ Dense(128) â†’ ReLU

FUSION:
  â”œâ”€ Concatenate([512, 128]) â†’ (640,)
  â”œâ”€ Dense(256) â†’ ReLU â†’ Dropout(0.3)
  â”œâ”€ Dense(128) â†’ ReLU â†’ Dropout(0.2)
  â”œâ”€ Dense(num_classes)
  â””â”€ CTC Decoding (best path or beam search)

Loss: CTC Loss
Optimizer: Adam (lr=1e-3)
```

---

## ğŸ”„ Workflow

### Phase 1: Data Collection & Preparation
1. Use MediaPipe Holistic to capture all landmarks
2. Store as .npy files with frame sequences
3. Verify data integrity and consistency

### Phase 2: Model Training
1. Load data in batches
2. Use CTC loss for continuous sequence modeling
3. Validate on held-out test set
4. Save best model weights

### Phase 3: Inference & Translation
1. **Sign â†’ Text:** 
   - Capture signer's gestures
   - Extract landmarks using MediaPipe
   - Feed through dual-stream model
   - Get CTC decoded text
   - Translate to target language (using Google API)

2. **Text/Speech â†’ Sign:**
   - Convert user speech to text (Google Speech-to-Text)
   - Translate to English (pivot language)
   - Generate synthetic sign video (gesture synthesis)
   - Display to signer

### Phase 4: Personalization
1. Collect 5-10 calibration samples from new user
2. Fine-tune last 2 layers with low learning rate
3. Store personalized weights
4. Use confidence threshold for continuous adaptation

---

## ğŸ› ï¸ Technical Stack

| Component | Technology |
|-----------|-----------|
| **Pose Estimation** | MediaPipe Holistic |
| **Hand Landmark Detection** | MediaPipe Hands (21 pts/hand) |
| **Facial Recognition** | MediaPipe Face (468 pts) |
| **Sequence Modeling** | LSTM (Bidirectional) |
| **Spatial Feature Extraction** | CNN (Conv1D) |
| **Sequence-to-Sequence** | CTC Loss |
| **Multilingual Translation** | Google Cloud Translation API |
| **Speech-to-Text** | Google Cloud Speech-to-Text API |
| **UI Framework** | Tkinter/Gradio (Frontend) |
| **Backend** | Flask/FastAPI |
| **ML Framework** | TensorFlow/Keras |

---

## ğŸ“ˆ Expected Performance Metrics

- **Frame-level Accuracy:** >95% (hand landmarks)
- **Sequence Accuracy:** >90% (continuous gestures)
- **Latency:** <500ms per gesture
- **Multilingual Support:** 5+ languages (via pivot)
- **Personalization Improvement:** +15-25% accuracy after 10 samples

---

## ğŸš€ Development Timeline

**Day 1:** Architecture setup, dual-stream model implementation, data pipeline
**Day 2:** Training pipeline with CTC loss, basic inference
**Day 3:** Multilingual translation integration
**Day 4:** Signer-adaptive learning implementation
**Day 5:** UI and presentation

---

## ğŸ“ Files Structure

```
OmniSign_Project/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dual_stream_model.py          # Main model architecture
â”‚   â”œâ”€â”€ ctc_loss.py                   # CTC loss implementation
â”‚   â””â”€â”€ model_utils.py                # Model utilities
â”œâ”€â”€ data_pipeline/
â”‚   â”œâ”€â”€ data_loader.py                # Data loading and preprocessing
â”‚   â”œâ”€â”€ feature_extractor.py          # MediaPipe landmark extraction
â”‚   â””â”€â”€ data_augmentation.py          # Data augmentation techniques
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ translator.py                 # Multilingual translation module
â”‚   â”œâ”€â”€ personalization.py            # Signer-adaptive learning
â”‚   â”œâ”€â”€ gesture_synthesis.py          # Sign video generation
â”‚   â””â”€â”€ confidence_handler.py          # Confidence-based error handling
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ app.py                        # Main application
â”‚   â””â”€â”€ components.py                 # UI components
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md               # This file
â”‚   â”œâ”€â”€ API_SPEC.md                   # API specifications
â”‚   â””â”€â”€ USER_GUIDE.md                 # User manual
â”œâ”€â”€ train_model.py                    # Training script
â”œâ”€â”€ predict_sign.py                   # Inference script
â”œâ”€â”€ main_app.py                       # Application entry point
â”œâ”€â”€ requirements.txt                  # Dependencies
â””â”€â”€ README.md                         # Project overview
```

---

## ğŸ” Ethical AI Considerations

1. **Confidence Scoring:** Display confidence threshold; ask for repeat if <85%
2. **Data Privacy:** User personalization data stored locally, not transmitted
3. **Bias Mitigation:** Collect diverse signing styles during data collection
4. **Accessibility:** Low-latency processing for real-time communication
5. **Transparency:** Show which features the model used for prediction

